---
title: "Vectors in ML | KNN"
format:
  html:
    code-fold: true
    mermaid-format: png
    mermaid:
      theme: default
#   html: default
#   typst: default
# jupyter: qmd_Py3111_trial
# jupyter: WinPy3.12sys
# jupyter: python3
jupyter:
  kernelspec:
    display_name: Python 3.12.0
    language: python
    name: python3
execute:
  cache: true
  warning: false
  fenced: true
code-link: true
code-block-border-left: true
fig-align: center
---
Vectors are defined as: "A quantity having direction as well as magnitude, especially as determining the position of one point in space relative to another."
  * ex: A two dimensional vector is given below:

$$
\text{2d vector} = \begin{bmatrix} x \\ y \end{bmatrix}
$$

- where x and y are the coordinates of the vector, relating to the number of elements it contains.
A two dimensional vector will have an $x$ and $y$ component. Vectors are written with a $\rightarrow$ arrow accent indicating we're dealing with a vector therefore the above would be written as:
$\vec{v} = \begin{bmatrix} x \\ y \end{bmatrix}$

In order to visualize what a two dimensional vector looks like, use the two examples below:
$$
\begin{align}
\vec{v}_{1} = \begin{bmatrix} 1 \\ 2 \end{bmatrix} \text{, } \vec{v}_{2} = \begin{bmatrix} 3 \\ 1 \end{bmatrix}
\end{align}
$$
```{python}
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
# mpl.rcParams['axes.autolimit_mode'] = 'round_numbers'
# mpl.rcParams['axes.xmargin'] = 0
# mpl.rcParams['axes.ymargin'] = 0

plt.style.use("seaborn-v0_8-pastel")

v1 = np.array([1, 2])
v2 = np.array([3, 1])

fig, ax = plt.subplots(figsize = (8, 6))


def plot_vec2d(vec2d, ax, color=np.random.random(3), **opts):
    return ax.arrow(
        0,
        0,
        vec2d[0],
        vec2d[1],
        head_width=0.3,
        head_length=0.3,
        length_includes_head=True,
        color=color,
        **opts,
    )

for vec in [v1, v2]:
    plot_vec2d(vec, ax = ax)
# plot_vec2d([v1, v2])
```
# Elementary Operations
---
---
## Addition
Two vectors result in a third vector. Taking the vectors we have been working with, $\vec{v}_{1}$ and $\vec{v}_{2}$, we can add them together to get a third vector, $\vec{v}_{3}$:

<!-- \vec{v}_{1} + \vec{v}_{2} = \begin{bmatrix} 1 \\ 2 \end{bmatrix} + \begin{bmatrix} 3 \\ 1 \end{bmatrix} = \begin{bmatrix} \text{1 + 3 =} 4 \\ \text{2 + 1 =} 3 \end{bmatrix} -->

$\begin{align}\vec{v}_{1} = {1 \brack 2} \text{, } \vec{v}_{2} = {3 \brack 1} \text{, } \vec{v}_{3} = \vec{v}_{1} + \vec{v}_{2} = {1 + 3 =  4\brack 2 + 1 = 3} \end{align}$

Notice due to ***commutivity*** the order of the vectors does not matter. The result will be the same.
```{python}
v3 = v1 + v2
print(v3)
```
## Subtraction
Subtraction is similar to addition, but instead of adding the vectors, we subtract them. Subtracting $\vec{v}_{2}$ from $\vec{v}_{1}$, we get a third vector, $\vec{v}_{4a}$:

$\begin{align}\vec{v}_{1} = {1 \brack 2} \text{, } \vec{v}_{2} = {3 \brack 1} \text{, } \vec{v}_{4a} = \vec{v}_{1} - \vec{v}_{2} = {1 - 3 =  -2\brack 2 - 1 = 1} \end{align}$
```{python}
v1 = np.array([1, 2])
v2 = np.array([3, 1])
v4a = v1 - v2
print(v4a)
```
Note: Vector subtraction isnt commutative. The order does matter when subtracting vectors.
Subtracting $\vec{v}_{1}$ from $\vec{v}_{2}$, we get a third vector, $\vec{v}_{4b}$:

$\begin{align}\vec{v}_{1} = {1 \brack 2} \text{, } \vec{v}_{2} = {3 \brack 1} \text{, } \vec{v}_{4b} = \vec{v}_{2} - \vec{v}_{1} = {3 - 1 =  2\brack 1 - 2 = -1} \end{align}$
```{python}
v4b = v2 - v1
print(v4b)
```
## Scalar Multiplication
Scalar multiplication is multiplying a vector by a scalar. Multiplying $\vec{v}_{1}$ by 2, we get a third vector, $\vec{v}_{5}$:

$\begin{align}2\vec{v}_{1} = 2{1 \brack 2} = {1 \times 2 = 2\brack 2 \times 2 = 4} \end{align}$
```{python}
scalar1 = 2
v1 * scalar1
```
Multiplying by a negative scalar has a similar interpretation, but the vector is scaled by the magnitude of the given scalar with the direction reversed.

$\begin{align}-1.5\vec{v}_{2} = -1.5{3 \brack 1} = {3 \times -1.5 = -4.5\brack 1 \times -1.5 = -1.5} \end{align}$

## Length of a Vector
The length of a vector is also known as the magnitude or `Euclidean norm` depending on the context. In order to calculate the length of a 2d vector the following formula is used:
$\begin{align} \text{length } \vec{v} = \sqrt{x^{2} + y^{2}} \end{align}$
The general formula for the length of an n-dimensional vector is given below. Note that $i$ below denotes the *index* of the $i^{th}$ element, therefore the first element would be $x$, the second
would be $y$, and the $n^{th}$ element is the last element in the vector.
The length of a vector $\vec{v}$ is denoted as $||\vec{v}||$.
$\begin{align}||\vec{v}|| = \sqrt{\sum_{i=1}^{n} \vec{v}_{i}^{2}}, \text{where } \vec{v} = \begin{bmatrix} v_{1} \\ v_{2} \\ â‹® \\ v_{n} \end{bmatrix}\end{align}$
```{python}
def vec_len(vec):
    return np.sqrt(np.sum([i**2 for i in vec]))
```
Example of vectors we have been using:
$\begin{align}\vec{v}_{1} = {1 \brack 2} \text{, } \vec{v}_{2} = {3 \brack 1} \end{align}$
```{python}
lv1 = vec_len(v1)
lv2 = vec_len(v2)
print(f"Length of v1: {lv1:2g}\nLength of v2: {lv2:2g}")
```
Taking the length of $\vec{v}_{1}$ and showing the geometric interpretation below, you can observe that the length of the vector is $\sqrt{5}$, which agrees with the output of the python function shown above. Notice the length as being the hypotenuse of a right angled triangle.

## Distance between vectors
$\begin{align}\text{distance (d) } = \sqrt{(x_{2} - x{1})^{2} + (y_{2} - y_{1})^{2}}\end{align}$
This can be generalized for an n-dimensional vector (utilizing vector subtraction):
$\begin{align}{d(\vec{v},\vec{u})}  = ||\vec{v} -\vec{u}|| \end{align}$
```{python}
vec_len(v1 - v2)
```
```{python}
vec_len(v1 - v2) == vec_len(v2 - v1) == np.sqrt(5)
```

<!-- The dot product of two vectors is the sum of the products of the corresponding elements of the two vectors. The dot product of two vectors $\vec{v}_{1}$ and $\vec{v}_{2}$ is given by:
$\begin{align}\vec{v}_{1} \cdot \vec{v}_{2} = \sum_{i=1}^{n} \vec{v}_{1i} \times \vec{v}_{2i}\end{align}$ -->

## K-Nearest Neighbors (KNN)
K-Nearest Neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure. It is mostly used for classification problems in Machine Learning. The algorithm works as follows:
1. Calculate the distance between the new data point and all the data points in the dataset.
2. Sort the data points based on the distance.
3. Select the top K data points.
4. Assign a class to the new data point based on the majority class of these data points.

Lets say we're given a dataset of cats and dogs. Cats = 0, Dogs = 1.
```{python}
animals = np.array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0])
```
These are the following weights and the amount of food consumed per month:
```{python}
weights = np.array([20, 15, 23, 17, 26, 18, 17, 20, 28, 19, 28, 21, 22, 19])
food_inpounds = np.array([6, 6, 4, 6, 12, 4, 9, 4, 6, 9, 12, 11, 8, 9])
```
Visualizing the dataset:
```{python}
colors = ['blue' if i == 1 else 'red' for i in animals]
fig, ax = plt.subplots(figsize = (10, 6))
plt.scatter(weights, food_inpounds, c=colors)
plt.xlabel('Animal Weight (Kg)', fontsize = 20)
plt.ylabel('Cats & Dogs', fontsize = 20)

import matplotlib.patches as mpatches

classes = ['cats', 'dogs', 'unknown']
class_colors = ['r', 'b', 'black']
recs = []
for i in range(len(class_colors)):
    recs.append(mpatches.Rectangle((0, 0), 1, 1, fc = class_colors[i]))
plt.legend(recs, classes, loc = 'best', prop = {'size': 15})
```
